# -*- coding: utf-8 -*-
"""Customer Segmentation Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sB94GS660e59cil5KEoE-OTGqt5B3BGF

## Customer Segmentation
"""

import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('Dataset.csv')
data

"""## Data Preprocessing"""

data.columns

data.info()

data.shape

data.describe()

data.nunique()

data.isnull().sum()

data_clean = data.copy()

for col in ['CREDIT_LIMIT','MINIMUM_PAYMENTS']:
    data_clean[col].fillna(data_clean[col].median(), inplace=True)
data_clean.head()

# Drop the CUST_ID column
data_clean.drop('CUST_ID', axis=1, inplace=True)

data_clean.head()

data_clean.isnull().any()

# Any duplicates
data_clean.duplicated().sum()

"""## Exploratory Data Analysis"""

data_clean.hist(figsize=(15,15))
plt.show()

data_clean.boxplot(rot=90, figsize=(30,20))
plt.show()

sns.pairplot(data_clean)
plt.show()

# Plot 1: Distribution of BALANCE to see its spread
plt.figure(figsize=(8, 6))
sns.histplot(data_clean['BALANCE'], kde=True, color='purple')
plt.title('Distribution of BALANCE')
plt.xlabel('BALANCE')
plt.ylabel('PURCHASE_FREQUENCY')
plt.show()

# Plot 2: Heatmap to visualize correlations
plt.figure(figsize=(12, 8))
sns.heatmap(data_clean.corr(), annot=True, cmap='coolwarm', linewidth=0.5)
plt.title('Correlation Heatmap of Features')
plt.show()

"""balance, purchases, credit_limit"""

sns.jointplot(x='BALANCE_FREQUENCY', y='BALANCE', data=data_clean, kind='scatter')

sns.jointplot(x='PURCHASES', y='ONEOFF_PURCHASES', data=data_clean, kind='scatter')

sns.jointplot(x='ONEOFF_PURCHASES', y='INSTALLMENTS_PURCHASES', data=data_clean, kind='scatter')

sns.jointplot(x='PURCHASES', y='PURCHASES_FREQUENCY', data=data_clean, kind='scatter')

sns.jointplot(x='PURCHASES', y='CASH_ADVANCE', data=data_clean, kind='scatter')

sns.relplot(x='PURCHASES', y='CREDIT_LIMIT', data=data_clean, kind='scatter')

"""

* high purchases in the range 0-10000
* Credit limit more in the range 0-20000


"""

sns.jointplot(x='CREDIT_LIMIT', y='BALANCE', data=data_clean, kind='scatter')

"""

* CREDIT_LIMIT & BALANCE are in a linear relation with each other ie. if a customer's balance increases, his/her credit limit shall increase

* Most customers lie in and under 15000 credit limit and 10K balance others can be considered as premium customers

"""

sns.jointplot(x='BALANCE_FREQUENCY', y='BALANCE', data=data_clean, kind='scatter')

sns.relplot(x='MINIMUM_PAYMENTS',y='BALANCE',data=data_clean,kind='scatter')

sns.jointplot(x='TENURE', y='PURCHASES', data=data_clean, kind='scatter')

sns.jointplot(x='TENURE', y='BALANCE', data=data_clean, kind='scatter')

"""* People with 12 as tenure tend to have more balance in their account


"""

sns.relplot(x='TENURE', y='PAYMENTS', data=data_clean, kind='scatter')

"""* more tenure more payments


"""

import plotly.express as py

plt.figure(figsize=(10,5))
py.scatter(data_clean, x='TENURE', y='CREDIT_LIMIT', color='TENURE')

"""* As the TENURE increases, CREDIT_LIMIT increases


"""

# Importing libraries for clustering
from sklearn.cluster import KMeans

# Using the elbow method to find the optimal number of clusters for KMeans
wcss = []  # Within-cluster sum of squares
K = range(1, 11)

# Calculating WCSS for different number of clusters
for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(data_clean)
    wcss.append(kmeans.inertia_)

# Plotting the elbow curve
plt.figure(figsize=(8, 6))
plt.plot(K, wcss, 'bo-', color='red')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('WCSS')
plt.title('Elbow Method for Optimal k')
plt.show()

"""

* **For K-Means**





"""

# Based on the elbow method, we choose the optimal number of clusters
optimal_k = 3

kmeans = KMeans(n_clusters=optimal_k, random_state=42)
clusters = kmeans.fit_predict(data_clean)
data_clean['Clusters'] = clusters

from sklearn.metrics import silhouette_score
score = silhouette_score(data_clean, clusters)
print('Silhouette Score:', score)

# Visualize the cluster centres if using KMeans

cluster_centres = pd.DataFrame(kmeans.cluster_centers_, columns=data_clean.columns[data_clean.columns != 'Clusters'])

plt.figure(figsize=(8,6))
sns.heatmap(cluster_centres.T, annot=True, cmap='coolwarm')
plt.title('Cluster Centres (K-means)')
plt.xlabel('Cluster')
plt.ylabel('Features')
plt.show()

# calculate the mean for each cluster
cluster_profiles = data_clean.groupby('Clusters').mean()

# Display the profile of each cluster
print(cluster_profiles)

# Plotting the profiles of the cluster for each feature

for column in data_clean.columns:
  plt.figure(figsize=(8,4))
  sns.barplot(x='Clusters', y=column, data=data_clean)
  plt.title(f'{column} by Clusters')
  plt.show()

for cluster in cluster_profiles.index:
  print(f"\nCluster {cluster}:")
  print(f"Characteristics:")
  # Prints the mean features of this cluster
  print(f"{cluster_profiles.loc[cluster]}")

  # Now summarize the marketing strategy for each cluster based on their profile
  if cluster_profiles.loc[cluster]['BALANCE'] > 0.5:
    print("Suggested Strategy: Offer premium rewards and perks for higher spenders.")
  elif cluster_profiles.loc[cluster]['PURCHASE'] < 0.2:
    print("Suggested Strategy: Incentivize engagement with cashback offers.")

sns.set(style='whitegrid')

# Plot balance distribution across clusters
plt.figure(figsize=(8,6))
sns.boxplot(x='Clusters', y='BALANCE', data=data_clean)
plt.title('Balance Distribution by Cluster')
plt.show()

# Plot purchase distribution across clusters
plt.figure(figsize=(8,6))
sns.boxplot(x='Clusters', y='PURCHASES', data=data_clean)
plt.title('Purchase Distribution by Cluster')
plt.show()

# Plot cash advance distribution across clusters
plt.figure(figsize=(8,6))
sns.boxplot(x='Clusters', y='CASH_ADVANCE', data=data_clean)
plt.title('Cash Advance Distribution by Cluster')
plt.show()

# Plot credit limit distribution across clusters
plt.figure(figsize=(8,6))
sns.boxplot(x='Clusters', y='CREDIT_LIMIT', data=data_clean)
plt.title('Credit Limit Distribution by Cluster')
plt.show()

"""# Principal Component Analysis(PCA)
```
We'll start by using PCA to reduce dimension
```
"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Standardizing the data
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_clean)

# Applying PCA
pca = PCA(n_components=0.95) # Retain 95% of the variance
data_pca = pca.fit_transform(data_scaled)

# Explained variance ratio to check how much information PCA retained
plt.figure(figsize=(10, 6))
plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Explained Variance by PCA Components')
plt.grid(True)
plt.show()

# Number of components chosen
print(f"Number of components after PCA: {pca.n_components_}")

"""* Clustering with PCA  



```
Now, let's see how K-means clustering performs on the PCA-transformed data. We'll do a comparison to see if reducing the dimensions helped or not
```



"""

plt.scatter(data_pca[:, 0], data_pca[:, 1], c=clusters, cmap='viridis', alpha=0.5)

# Adding cluster to PCA-transformed data for visualization
data_pca_df = pd.DataFrame(data_pca, columns=[f'PC{i+1}' for i in range(pca.n_components_)])
data_pca_df['Clusters'] = clusters_pca

# Visualize PCA clusters
plt.figure(figsize=(10, 6))
plt.scatter(data_pca[:, 0], data_pca[:, 1], c=clusters, cmap='viridis', alpha=0.5)
plt.title('PCA Cluster Visualization')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(label='Cluster')
plt.show()

# Clustering on PCA-reduced data
kmeans_pca = KMeans(n_clusters=optimal_k, random_state=42)
clusters_pca = kmeans_pca.fit_predict(data_pca)

# Adding clusters to the dataset for visualization
data_clean['Cluster_PCA'] = clusters_pca

# Silhouette score to evaluate clustering performance after PCA
from sklearn.metrics import silhouette_score
silhouette_pca = silhouette_score(data_pca, clusters_pca)
print(f"Silhouette Score after PCA: {silhouette_pca}")

# Compare silhouette scores for PCA and original data
silhouette_original = silhouette_score(data_clean, clusters)
silhouette_pca = silhouette_score(data_pca, clusters_pca)

print(f"Silhouette Score before PCA: {silhouette_original}")
print(f"Silhouette Score after PCA: {silhouette_pca}")

# Cluster Profiling
cluster_profiles = pd.DataFrame(kmeans_pca.cluster_centers_, columns=[f'PC{i+1}' for i in range(data_pca.shape[1])]) # Create column names for the principal components
print(cluster_profiles)

# Marketing strategies based on clusters
for cluster in cluster_profiles.index:
    if cluster_profiles.loc[cluster]['PC1'] > 0.5:
        print(f"\nCluster {cluster}:Offer premium rewards and perks for higher spenders.")
    else:
      print(f"\nCluster {cluster}: Target low engagement with cashback incentives.")

# Cluster Profiling
cluster_means = data_clean.groupby('Clusters').mean()
print(cluster_means)

# Extended marketing strategy
for cluster in cluster_means.index:
  print(f"\nCluster {cluster}:")
  print(f"Cluster Characteristics:\n{cluster_profiles.loc[cluster]}")

  if cluster_profiles.loc[cluster]['BALANCE'] > 0.5 and cluster_profiles.loc[cluster]['CREDIT_LIMIT'] > 0.5:
    print(f"Cluster {cluster}:Target premium customers with high rewards programs and luxury offers.")
  elif cluster_profiles.loc[cluster]['TENURE'] > 0.5 and cluster_profiles.loc[cluster]['PC1'] < 0.3:
    print(f"Cluster {cluster}: Incentivize longer-term customers with loyalty points or free waivers.")
  else:
    print(f"Cluster {cluster}: Focus on engagement with cashback offers and credit building incentives.")